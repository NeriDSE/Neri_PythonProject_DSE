The BoardGameGeek (BGG) website provides various data and statistics on the enthusiastic
board game hobby. BGG users comment on games and optionally assign them a liking score
between 0 and 10.

The dataset contains millions of board game reviews. The purpose of the project is to
produce a ranking of games in order of user liking. 

It should be noted that in order to obtain
an adequate ranking it is necessary to consider not only the votes, but also the fact that
different games may be associated with very different numbers of comments and thus votes
from individual users. For a discussion of this point see, for example, How Not To Sort By
Average Rating.

The project should propose its own strategy for sorting the games, also arguing its
appropriateness in relation to the official BGG ranking, available online.

The project should also produce graphs comparing the proposed ranking with the ranking
obtained only through the average rating of each game, showing the different distribution of
scores and the differences found.


What BGG says on the ratings: (https://boardgamegeek.com/wiki/page/ratings)
https://boardgamegeek.com/blog/10129/blogpost/109537/reverse-engineering-the-boardgamegeek-ranking revealing the secret sauce behindd the BGG rankings.

The User Ratings are also used to determine the Rank of a game in the BGG database.

Only games that have at least 30 User Ratings are eligible for Ranking and to the User Ratings are added a number of "dummy" ratings, which are then used to produce a new average Geek Rating. (E.g. see this thread.) This is the rating that shows up in BGG searches and the number can, and does, vary from the Average Rating. It may be that once a game is in the database for a year or so, it also gets a geek rating even with fewer ratings, based on this thread.

In effect the "dummy" ratings move a game's average towards the norm of all games on the database - making games with few votes but very high ratings lower ranked than games with many more ratings but a lower Average Rating. (If you want to know more about this process, search on "Bayesian" within BGG.) Additionally, secret undocumented stuff is done to try to filter out obviously bogus "shill" or "hate" ratings. And Aldie has said that subdomain ranks are calculated using a subset of users who tend to rate games of that subdomain. (There are many threads from people asking about or trying to figure out the details, but they are intentionally undocumented.)

To get the Bayesian average rating of a game, take all of the real votes, add in 30 imaginary votes of about 5.8 (the average vote on the BGG), and average the whole lot of them. This has the effect of pulling the ratings of games with not so many votes toward the average rating. It's as simple as that. It keeps a game that is very popular with a designer and 29 of his friends (or just a small group of people) from immediately rising to the top of the rankings.


# # The BoardGameGeek (BGG) (https://boardgamegeek.com/wiki/page/Guide_to_Database_Corrections#toc18) provides various data and statistics 
# on the enthusiastic board game hobby. 
# BGG users comment on games and optionally assign them a liking score between 0 and 10.

# The PURPOSE of the project is to produce a ranking of games in order of user liking.
 
# 1. Rank the dataset in a traditional sense
# 
# 
# 2. It should be noted that in order to obtain an adequate ranking it is necessary to consider not only the votes, 
# but also the fact that different games may be associated with very different numbers of comments and thus
# votes from individual users. 
# For a discussion of this point see, for example, How Not To Sort By Average Rating.

# 3. The project should propose its OWN strategy for sorting the games.
# also arguing its appropriateness in relation to the official BGG ranking, available online.

# 4. The project should also produce graphs comparing the proposed ranking with the ranking obtained 

# only through the average rating of each game, showing the different distribution of scores and the differences found.

Further thoughts:

How does GBB rank its games? This is what I'm going up against. It's a bit of a secret sauce. 
In the bayesian way, that is
sum of scores + number of dummies * dummy average score / average true score - dummy average score.
how many dummies (it varies) and what is the average score (unclear)

I have to implement my way of ranking these  games. Well I think I can implement several and compare 
the various ways.
My ways vs average rating vs the Bayesian "Geek Rating" rating with graphs. cool.

join the data, configure the data in the right way, find the wilson scores through numpy probably, 
graph the results, graph the other results, compare with bayesian and graph again
maybe find yet another way of handling the data.



Let's see if I have to come with mine, their rating is the bayesian updating rating, based on adding dummies to prevent games from skyrocketing to the top. 

let's have a free write on other possible ideas for ranking games.
BGG's concept is balancing new hotness with unddying classics, I could play with the year it came out, I would segment the games based on the genre. Children's should just not really be in the same place as all the other ones. 
I've yet to rank all the other ones.

balancing the ranks
other types of weighted averages, like at different levels
wilson score with a different positive share.

another option is going in on the details of the games

If I were to rank something, how would I rank it? This is all about rankig. so I would rank things based on how niche and insane something was, how creative and innovative. A few interesting ideas are looking at gameowners who have many games, like mother said, and ranking them through bayesian average with that. I feel like that could be cool. but the algorithm remains the same.
Otherwise there's the sorting by the vote but also the length of the comments and if the comments include any interesting vocabulary.
I have to import the functions and call them into another file.

I can just include many of these to check.
For algorithms it's a matter of pairing the average with some other feature.

i feel like the wilson score could be weighed somehow.
I'm gonna try and do all of them. TOday Bayesian all the way with the graphs. Then I can add filters like non prolific authors and such

comments and reliability.
I guess it can be a combination of various things, wilson score of many people who have posted and commented on many games.
so... let's try to first implement the Bayesian equation with the graphs
